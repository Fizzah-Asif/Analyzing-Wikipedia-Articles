{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0bb3bd8",
   "metadata": {},
   "source": [
    "# STEPS:\n",
    "# ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3ba2f",
   "metadata": {},
   "source": [
    "# 1. Including the article in our program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f3aba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os                                                                    #os provides us with functions to work with directories/folders\n",
    "filePath = r\"C:\\Users\\Administrator\\Rufus Thomas\"                            #path where the article is placed\n",
    "p = [os.path.join (filePath, name) for name in os.listdir(filePath)]         #gets all the files in the foler (in this case 1)\n",
    "L_file = []\n",
    "\n",
    "for pth in p:                                                                #loop to traverse through the entire article\n",
    "    with open (pth, 'r', encoding = \"ISO-8859-1\") as FL:\n",
    "        FE = FL.readlines()                                                  #will return each line of the article\n",
    "        L_file.append(FE)                                                    #adds the article to the list 'file'\n",
    "\n",
    "L_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32342ec1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now converting the list 'file' into string\n",
    "\n",
    "strFile = ' '.join(map(str,L_file))             #this will take all items in an iterable and join them into one string\n",
    "strFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing / Tokenizing the article into seperate words\n",
    "\n",
    "%pip install nltk                         # installing Natural Language ToolKit since it is used to analyze human language data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords         # 'corpus' = collection of text docs , 'stopwords' = words in a sentence that are not needed that much\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')                    # for dividing the text into a list of sentences\n",
    "from nltk.tokenize import word_tokenize   \n",
    "w = word_tokenize(strFile)                # dividing the text into seperate words\n",
    "data = [word for word in w if not word in stopwords.words()]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the words of the similar context\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "Lmt = WordNetLemmatizer()\n",
    "L = []\n",
    "for i in data:\n",
    "    L.append(Lmt.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "upd = [re.sub('[^a-zA-Z0-9]+ ', '', _) for _ in Lmt]\n",
    "upd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b99adf",
   "metadata": {},
   "source": [
    "# 2. Clustering using sklearn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5068e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer   # to check frequency of a word\n",
    "from sklearn.cluster import KMeans                            # to make clusters\n",
    "from sklearn.metrics import adjusted_rand_score               # to identify similarities between clusters\n",
    "from warnings import simplefilter                             # to ignore future warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d729b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(action = 'ignore', category = FutureWarning)\n",
    "vec = TfidfVectorizer(stop_words = 'english')\n",
    "tf = vec.fit_transform(upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 4                            # this will make 4 clusters\n",
    "cl = KMeans(n_clusters = cluster, init = \"k-means++\", max_iter = 100, n_init = 1)\n",
    "cl.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c95529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will print all the clusters\n",
    "\n",
    "print(\"Clusters are formed as follows:\\n\")\n",
    "cntr = cl.cluster_centers_.argsort()[:,::-1]\n",
    "nm = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range (cluster):\n",
    "    print(\"Cluster %d:\" %i)\n",
    "    for i in cntr[i,:10]:\n",
    "        print(' %s' %nm[i])\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee985ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
